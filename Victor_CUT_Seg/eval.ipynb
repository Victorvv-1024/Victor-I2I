{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vw/anaconda3/envs/i2i/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from utils import util\n",
    "from models.cut_seg import CUT_SEG_model\n",
    "import torch\n",
    "from utils.create_dataset import EczemaDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> pick 100 images from source and target folder for evaluation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = 'evaluation/source'\n",
    "source_img_path = []\n",
    "\n",
    "for ext in ('*.jpg', '*.png', '*.JPG', '*.PNG'):\n",
    "    source_img_path.extend(glob(os.path.join(source_path, ext)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_img_path = random.sample(source_img_path, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "saved_source_path = 'evaluation/clean_source'\n",
    "for i, path in enumerate(source_img_path):\n",
    "    shutil.copy(path, f'{saved_source_path}/source={i + 1}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = 'evaluation/target'\n",
    "target_img_path = []\n",
    "\n",
    "for ext in ('*.jpg', '*.png', '*.JPG', '*.PNG'):\n",
    "    target_img_path.extend(glob(os.path.join(target_path, ext)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img_path = random.sample(target_img_path, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_target_path = 'evaluation/clean_target'\n",
    "for i, path in enumerate(target_img_path):\n",
    "    shutil.copy(path, f'{saved_target_path}/target={i + 1}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> plot the loss curve </h1></br>\n",
    "<h4> plot netG loss </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_path = 'output_HPC/victor_demo_v4/loss/loss_400'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "with (open(loss_path, \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            loss.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n",
    "loss = loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_G = [l['G'] for l in loss]\n",
    "loss_GAN = [l['G_GAN'] for l in loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_G)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('netG_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_GAN)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('netG_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load the network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArgParse():\n",
    "    parser = argparse.ArgumentParser(description='CUT inference usage.')\n",
    "    # Evaluation\n",
    "    # model parameters\n",
    "    \"\"\"GAN parameters\"\"\"\n",
    "    parser.add_argument('--CUT_mode', type=str, default=\"CUT\", choices=['CUT', 'cut', 'FastCUT', 'fastcut'], help='')\n",
    "    parser.add_argument('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')\n",
    "    parser.add_argument('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')\n",
    "    parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')\n",
    "    parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')\n",
    "    parser.add_argument('--netD', type=str, default='basic', choices=['basic', 'n_layers'], help='specify discriminator architecture. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')\n",
    "    parser.add_argument('--netG', type=str, default='resnet_9blocks', choices=['resnet_9blocks', 'resnet_6blocks'], help='specify generator architecture')\n",
    "    parser.add_argument('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n",
    "    parser.add_argument('--normG', type=str, default='instance', choices=['instance', 'batch', 'none'], help='instance normalization or batch normalization for G')\n",
    "    parser.add_argument('--normD', type=str, default='instance', choices=['instance', 'batch', 'none'], help='instance normalization or batch normalization for D')\n",
    "    parser.add_argument('--init_type', type=str, default='xavier', choices=['normal', 'xavier', 'kaiming', 'orthogonal'], help='network initialization')\n",
    "    parser.add_argument('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n",
    "    parser.add_argument('--no_dropout', type=util.str2bool, nargs='?', const=True, default=True,\n",
    "                        help='no dropout for the generator')\n",
    "    parser.add_argument('--antialias', action='store_true', help='if specified, use antialiased-downsampling')\n",
    "    parser.add_argument('--antialias_up', action='store_true', help='if specified, use [upconv(learned filter)]')\n",
    "    parser.add_argument('--lambda_GAN', type=float, default=1.0, help='weight for GAN lossï¼šGAN(G(X))')\n",
    "    \"\"\"netF paramters\"\"\"\n",
    "    parser.add_argument('--lambda_NCE', type=float, default=1.0, help='weight for NCE loss: NCE(G(X), X)')\n",
    "    parser.add_argument('--nce_idt', type=util.str2bool, nargs='?', const=True, default=True, help='use NCE loss for identity mapping: NCE(G(Y), Y))')\n",
    "    parser.add_argument('--nce_layers', type=str, default='0,3,5,7,11', help='compute NCE loss on which layers')\n",
    "    parser.add_argument('--netF', type=str, default='mlp_sample', choices=['sample', 'reshape', 'mlp_sample'], help='how to downsample the feature map')\n",
    "    parser.add_argument('--netF_nc', type=int, default=256)\n",
    "    parser.add_argument('--nce_T', type=float, default=0.07, help='temperature for NCE loss')\n",
    "    parser.add_argument('--num_patches', type=int, default=256, help='number of patches per layer')\n",
    "    \"\"\"netS parameters\"\"\"\n",
    "    parser.add_argument('--netS', type=str, default='resnet', choices=['resnet', 'unet_256', 'smp'], help='how to segment the input image')\n",
    "    parser.add_argument('--smp_arch', type=str, default='Unet', help='the segmentor architectur')\n",
    "    parser.add_argument('--smp_encoder', type=str, default='efficientnet-b3', help='the encoder name')\n",
    "    parser.add_argument('--normS', type=str, default='instance', choices=['instance', 'batch', 'none'], help='instance normalization or batch normalization for S')\n",
    "    parser.add_argument('--num_class', type=int, default=2, help='# of output image channels for segmented mask')\n",
    "    parser.add_argument('--netS_lambda', type=int, default=10, help='lambda for SEG loss')\n",
    "    parser.add_argument('--netS_Loss', type=str, help='semantic segmentation loss function', choices=['dice', 'bce', 'DICE', 'BCE'], default='bce')\n",
    "    parser.add_argument('--flip_equivariance',\n",
    "                        type=bool, nargs='?', default=False,\n",
    "                        help=\"Enforce flip-equivariance as additional regularization. It's used by FastCUT, but not CUT\")\n",
    "\n",
    "    parser.add_argument('--src_dir', help='source dataset folder', type=str, default='evaluation/clean_source')\n",
    "    parser.add_argument('--out_dir', help='output folder', type=str, default='evaluation/v8/')\n",
    "    parser.add_argument('--name', type=str, default='demo_v4', help='name of the experiment. It decides where to store samples and models')\n",
    "    parser.add_argument('--easy_label', type=str, default='demo_v4', help='Interpretable name')\n",
    "    parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
    "\n",
    "    \n",
    "    parser.add_argument('--isTrain', type=util.str2bool, default=False)\n",
    "\n",
    "\n",
    "    opt, _ = parser.parse_known_args()\n",
    "\n",
    "    # Set default parameters for CUT and FastCUT\n",
    "    if opt.CUT_mode.lower() == \"cut\":\n",
    "        parser.set_defaults(nce_idt=True, lambda_NCE=1.0)\n",
    "    elif opt.CUT_mode.lower() == \"fastcut\":\n",
    "        parser.set_defaults(\n",
    "            nce_idt=False, lambda_NCE=10.0, flip_equivariance=True,\n",
    "            n_epochs=150, n_epochs_decay=50\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(opt.CUT_mode)\n",
    "\n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(CUT_mode='CUT', input_nc=3, output_nc=3, ngf=64, ndf=64, netD='basic', netG='resnet_9blocks', n_layers_D=3, normG='instance', normD='instance', init_type='xavier', init_gain=0.02, no_dropout=True, antialias=False, antialias_up=False, lambda_GAN=1.0, lambda_NCE=1.0, nce_idt=True, nce_layers='0,3,5,7,11', netF='mlp_sample', netF_nc=256, nce_T=0.07, num_patches=256, netS='resnet', smp_arch='Unet', smp_encoder='efficientnet-b3', normS='instance', num_class=2, netS_lambda=10, netS_Loss='bce', flip_equivariance=True, src_dir='evaluation/clean_source', out_dir='evaluation/v8/', name='demo_v4', easy_label='demo_v4', checkpoints_dir='./checkpoints', isTrain=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = ArgParse()\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = CUT_SEG_model(opt=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>load generator</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = getattr(model, 'netG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(netG, torch.nn.DataParallel):\n",
    "    netG = netG.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = 'checkpoints_HPC/demo_v8/400_net_G.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(load_path, map_location=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(state_dict, '_metadata'):\n",
    "    del state_dict._metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> create the dataset </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from utils.util import img2tensor\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class EvaluationDS(data.Dataset):\n",
    "    \n",
    "    def __init__(self, src_img_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.src_img_path = []\n",
    "        for ext in ('*.jpg', '*.png', '*.JPG', '*.PNG'):\n",
    "            self.src_img_path.extend(glob(os.path.join(src_img_path, ext)))\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_img_path)\n",
    "    \n",
    "    \n",
    "    def transform(self, real_img):\n",
    "        # Resize\n",
    "        resize = T.Resize(size=(286,286), interpolation=T.InterpolationMode.NEAREST)\n",
    "        real_img = resize(real_img)\n",
    "\n",
    "        # Random crop\n",
    "        i, j, h, w = T.RandomCrop.get_params(\n",
    "            real_img, output_size=(256, 256))\n",
    "        real_img = TF.crop(real_img, i, j, h, w)\n",
    "        \n",
    "\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            real_img = TF.hflip(real_img)\n",
    "            \n",
    "\n",
    "        return real_img\n",
    "    \n",
    "    def normalize(self, real_img):\n",
    "\n",
    "        normalize = T.Normalize(mean=(127.5,127.5,127.5), std=(127.5,127.5,127.5))\n",
    "        normalize_real_img = normalize(real_img)\n",
    "        normalize_real_img = normalize_real_img.squeeze(0)\n",
    "        \n",
    "        return normalize_real_img\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src_img = np.asarray(Image.open(self.src_img_path[index]))\n",
    "        src_img = img2tensor(src_img)\n",
    "        \n",
    "        t_src_img = self.transform(src_img)\n",
    "\n",
    "        # normalize the imgs\n",
    "        n_src_img = self.normalize(t_src_img)\n",
    "        \n",
    "        return n_src_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evaluation/clean_source'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.src_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = EvaluationDS(opt.src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dataloader = DataLoader(source, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> generate the translated image and save </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_path = os.path.join(opt.out_dir, 'compare')\n",
    "for idx, data in enumerate(src_dataloader):\n",
    "    # only translate 100 images\n",
    "    data = data.to(device)\n",
    "    # source\n",
    "    source_img = util.tensor2img(data)\n",
    "    source_img = (source_img * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "    # translated\n",
    "    translated = netG(data)\n",
    "    translated = util.tensor2img(translated)\n",
    "    translated = (translated * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "    _, ax = plt.subplots(1, 2, figsize=(20, 20))\n",
    "    ax[0].imshow(source_img)\n",
    "    ax[1].imshow(translated)\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[1].set_title(\"Translated\")\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    plt.savefig(f'{compare_path}/infer={idx + 1}.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "translated_path = os.path.join(opt.out_dir, 'translated')\n",
    "for idx, data in enumerate(src_dataloader):\n",
    "    data = data.to(device)\n",
    "\n",
    "    # translated\n",
    "    translated = netG(data)\n",
    "    translated = util.tensor2img(translated)\n",
    "    translated = (translated * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(translated)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.savefig(f'{translated_path}/infer={idx + 1}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('i2i')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ed0a27da37374a68e5786d5e28bc6290d773021d6d03647be9d095d4987b83b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
